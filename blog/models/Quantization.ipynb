{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22eb601e-a501-40c1-8de2-ac3d7a9a2f5d",
   "metadata": {},
   "source": [
    "This notebook is under progress.\n",
    "\n",
    "The purpose of the notebook is to demonstrate quantization of a deep learning model (ResNet in this example). Quantization is a method to reduce the number of bits used to represent each parameter in the model. There are two main purposes of quantization:\n",
    "1. Reduce Model Size\n",
    "Instead of using 32-bit floating-point numbers (FP32), quantization typically reduces this to 16-bit floating-point (FP16). This leads to significant reductions in the model's memory usage. Smaller models require less storage space, which is beneficial for deploying models on devices with limited memory, such as embedded systems.\n",
    "2. Improve Computational Efficiency\n",
    "Operations involving lower-bit integers (e.g., 8-bit integers) are typically faster to execute than those involving floating-point numbers. Hardware accelerators like CPUs, GPUs, and specialized AI processors often have optimized instructions for integer arithmetic, making quantized models more efficient in terms of computation. Lower precision data requires less bandwidth, which can be advantageous for data transfer and network communication in distributed systems or edge devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2390a954-f45d-4ace-bbe7-057312401120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88d48991-7d4e-4a7a-a14c-3c064337b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, w, bit, symmetric=False):\n",
    "        if bit is None:\n",
    "            wq = w\n",
    "        elif bit==0:\n",
    "            wq = w*0\n",
    "        else:\n",
    "            # Build a mask to record position of zero weights\n",
    "            weight_mask = (w!= 0).int()\n",
    "            if symmetric == False:\n",
    "                # Compute alpha (scale) for dynamic scaling\n",
    "                alpha = torch.max(w) - torch.min(w)\n",
    "                # Compute beta (bias) for dynamic scaling\n",
    "                beta = torch.min(w)\n",
    "                # Scale w with alpha and beta so that all elements in ws are between 0 and 1\n",
    "                ws = (w-beta)/alpha\n",
    "                step = 2 ** (bit)-1\n",
    "                # Quantize ws with a linear quantizer to \"bit\" bits\n",
    "                R = torch.round(step*ws)/step\n",
    "                # Scale the quantized weight R back with alpha and beta\n",
    "                wq = alpha*R+beta\n",
    "            else:\n",
    "                alpha = torch.max(torch.abs(w))\n",
    "                ws = w/alpha\n",
    "                step = 2**(bit-1)-1\n",
    "                R = torch.round(step*ws)/step\n",
    "                wq = alpha*R\n",
    "            # Restore zero elements in wq \n",
    "            wq = wq*weight_mask\n",
    "        return wq\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        return g, None, None\n",
    "\n",
    "class FP_Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, Nbits=None, symmetric=False):\n",
    "        super(FP_Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.Nbits = Nbits\n",
    "        self.symmetric = symmetric\n",
    "        \n",
    "        m = self.in_features\n",
    "        n = self.out_features\n",
    "        self.linear.weight.data.normal_(0, math.sqrt(2. / (m+n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, STE.apply(self.linear.weight, self.Nbits, self.symmetric), self.linear.bias)\n",
    "\n",
    "class FP_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False, Nbits=None, symmetric=False):\n",
    "        super(FP_Conv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.Nbits = Nbits\n",
    "        self.symmetric = symmetric\n",
    "\n",
    "        n = self.kernel_size * self.kernel_size * self.out_channels\n",
    "        m = self.kernel_size * self.kernel_size * self.in_channels\n",
    "        self.conv.weight.data.normal_(0, math.sqrt(2. / (n+m) ))\n",
    "        self.sparsity = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, STE.apply(self.conv.weight, self.Nbits, self.symmetric), self.conv.bias, self.conv.stride, self.conv.padding, self.conv.dilation, self.conv.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69485a4-5f13-45ca-950f-aae447bd7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, Nbits=None, symmetric=False):\n",
    "    super(ResidualBlock, self).__init__()\n",
    "    self.conv1 = FP_Conv(in_channels, out_channels, 3, stride=stride, padding=1, bias=False, Nbits=Nbits, symmetric=symmetric)\n",
    "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "    self.conv2 = FP_Conv(out_channels, out_channels, 3, stride=1, padding=1, bias=False, Nbits=Nbits, symmetric=symmetric)\n",
    "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    self.downsample = None\n",
    "    if stride==2:\n",
    "      self.downsample = FP_Conv(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "\n",
    "  def forward(self, x):\n",
    "    res = x\n",
    "    out = self.conv1(x)\n",
    "    out = self.bn1(out)\n",
    "    out = F.relu(out)\n",
    "    out = self.conv2(out)\n",
    "    out = self.bn2(out)\n",
    "    if self.downsample is not None:\n",
    "      res = self.downsample(x)\n",
    "    out += res\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "class ResNet20(nn.Module):\n",
    "  def __init__(self, resblock, n=3,  Nbits=None, symmetric=False):\n",
    "    super(ResNet20, self).__init__()\n",
    "    self.n = n\n",
    "    self.conv1 = FP_Conv(3, 16, 3, stride=1, padding=1, bias=False, Nbits=Nbits, symmetric=symmetric)\n",
    "    self.bn1 = nn.BatchNorm2d(16)\n",
    "    self.avgpool = nn.AvgPool2d(8)\n",
    "    self.fc1   = FP_Linear(64, 10, Nbits=None)\n",
    "    self.layer1 = self.create_layer(resblock, 16, 16, stride=1)\n",
    "    self.layer2 = self.create_layer(resblock, 16, 32, stride=2)\n",
    "    self.layer3 = self.create_layer(resblock, 32, 64, stride=2)\n",
    "\n",
    "  def create_layer(self, resblock, in_channels, out_channels, stride):\n",
    "    layers = nn.ModuleList()\n",
    "    layers.append(resblock(in_channels, out_channels, stride))\n",
    "    for i in range(self.n-1):\n",
    "      layers.append(resblock(out_channels, out_channels, stride=1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.conv1(x)\n",
    "    out = self.bn1(out)\n",
    "    out = F.relu(out)\n",
    "    out = self.layer1(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = self.avgpool(out)\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = self.fc1(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6d59e46-0802-4123-98dc-d1f96bdbc91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop(size=(32, 32), padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "])\n",
    "\n",
    "CIFAR10_train = torchvision.datasets.CIFAR10(root='CIFAR10_data/',\n",
    "                                   train=True,\n",
    "                                   transform=train_transform,\n",
    "                                   download=True)\n",
    "\n",
    "CIFAR10_test = torchvision.datasets.CIFAR10(root='CIFAR10_data/',\n",
    "                         train=False,\n",
    "                         transform=test_transform,\n",
    "                         download=True)\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=CIFAR10_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=CIFAR10_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def finetune(model, epochs, train_loader, test_loader, lr, reg, device, verbose=True):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.875, weight_decay=reg, nesterov=False)\n",
    "    for epoch in range(epochs):\n",
    "        # print('\\nEpoch: %d' % epoch)\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        # print(\"Training accuracy: %.4f\" % (correct/total))\n",
    "        # print(\"Training loss: %.4f\" % (train_loss/len(test_loader)))\n",
    "    model.eval()\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            out = model(inputs)\n",
    "            loss = criterion(out, targets)\n",
    "            total_test_loss += loss.item()\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total_examples += targets.size(0)\n",
    "            correct_examples += (predicted == targets).sum().item()\n",
    "    test_avg_acc = correct_examples / total_examples\n",
    "    test_avg_loss = total_test_loss / len(test_loader)\n",
    "    if verbose:\n",
    "        print(\"Test accuracy: %.4f\" % (test_avg_acc))\n",
    "        print(\"Test loss: %.4f\" % (test_avg_loss))\n",
    "    return test_avg_acc, test_avg_loss\n",
    "            \n",
    "\n",
    "def test_CIFAR10(model, test_loader, device, verbose=True):\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    model.eval()\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            out = model(inputs)\n",
    "            loss = criterion(out, targets)\n",
    "            total_test_loss += loss.item()\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total_examples += targets.size(0)\n",
    "            correct_examples += (predicted == targets).sum().item()\n",
    "    test_avg_acc = correct_examples / total_examples\n",
    "    test_avg_loss = total_test_loss / len(test_loader)\n",
    "    if verbose:\n",
    "        print(\"Test accuracy: %.4f\" % (test_avg_acc))\n",
    "        print(\"Test loss: %.4f\" % (test_avg_loss))\n",
    "    return test_avg_acc, test_avg_loss\n",
    "    \n",
    "def plot_acc(x, acc, x_label, y_label, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, acc, 'b-')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453d7b8-363c-4fbc-a94e-ac9635ad3097",
   "metadata": {},
   "source": [
    "## Fixed-point quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "949ad5df-6240-467f-82bf-4adc9df05191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Bits:  6\n",
      "Test accuracy: 0.8321\n",
      "Test loss: 0.7341\n",
      "Test Accuracy:  0.8321\n",
      "Test accuracy: 0.8590\n",
      "Test loss: 0.4339\n",
      "After-Finetune Test Accuracy:  0.859\n",
      "# Bits:  5\n",
      "Test accuracy: 0.8340\n",
      "Test loss: 0.7333\n",
      "Test Accuracy:  0.834\n",
      "Test accuracy: 0.8594\n",
      "Test loss: 0.4282\n",
      "After-Finetune Test Accuracy:  0.8594\n",
      "# Bits:  4\n",
      "Test accuracy: 0.8286\n",
      "Test loss: 0.7515\n",
      "Test Accuracy:  0.8286\n",
      "Test accuracy: 0.8591\n",
      "Test loss: 0.4336\n",
      "After-Finetune Test Accuracy:  0.8591\n",
      "# Bits:  3\n",
      "Test accuracy: 0.8297\n",
      "Test loss: 0.7507\n",
      "Test Accuracy:  0.8297\n",
      "Test accuracy: 0.8554\n",
      "Test loss: 0.4413\n",
      "After-Finetune Test Accuracy:  0.8554\n",
      "# Bits:  2\n",
      "Test accuracy: 0.7896\n",
      "Test loss: 0.9547\n",
      "Test Accuracy:  0.7896\n",
      "Test accuracy: 0.8564\n",
      "Test loss: 0.4340\n",
      "After-Finetune Test Accuracy:  0.8564\n",
      "# Bits:  1\n",
      "Test accuracy: 0.4121\n",
      "Test loss: 4.0464\n",
      "Test Accuracy:  0.4121\n",
      "Test accuracy: 0.8520\n",
      "Test loss: 0.4448\n",
      "After-Finetune Test Accuracy:  0.852\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "nbits_list = [6,5,4,3,2,1]\n",
    "for nbits in nbits_list:\n",
    "    model = ResNet20(ResidualBlock, n=3, Nbits=nbits, symmetric=False)\n",
    "    model = model.to(device)\n",
    "    model.load_state_dict(torch.load(\"resnet_quantization.pth\")['state_dict'])\n",
    "    print('# Bits: ', nbits)\n",
    "    print('Test Accuracy: ', test_CIFAR10(model, test_loader, device)[0])\n",
    "    test_avg_acc, test_avg_loss = finetune(model, epochs=20, train_loader=train_loader, test_loader=test_loader, lr=0.002, reg=1e-4, device=device, verbose=False)\n",
    "    print('After-Finetune Test Accuracy: ', test_avg_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10887ff-b67c-477a-a8ba-1251b6c25a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f512181-b8ca-452c-bdc8-7cc40c3ed514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet20(ResidualBlock, n=3, Nbits=1, symmetric=False)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"resnet_quantization.pth\")['state_dict'])\n",
    "for param in model.parameters():\n",
    "  print(param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847972d-b823-4c2a-9afa-19aa1738da44",
   "metadata": {},
   "source": [
    "## Symmetric quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf761f92-61d7-4cee-9f4a-edcb0f54ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbits = 4\n",
    "\n",
    "model = ResNetCIFAR(num_layers=20, Nbits=Nbits, symmetric=False)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"resnet_quantization.pth\"))\n",
    "print('Test Accuracy: ', test_CIFAR10(model, test_loader, device)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e71cbd-fc5a-4919-a078-89b6705fa4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe54a61f-ca01-4c9c-ac99-79139bdd3ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be8994-659f-4d25-8a60-7cfdbefd6226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfff074-cf5f-4ad5-b755-0d2ca693fda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e7e71-68f5-4969-ae54-e5a9208d0f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abe8e9-10ec-491e-aa52-11cb8c89f4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
