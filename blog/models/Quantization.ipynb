{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22eb601e-a501-40c1-8de2-ac3d7a9a2f5d",
   "metadata": {},
   "source": [
    "The purpose of the notebook is to demonstrate quantization of a deep learning model (ResNet in this example). Quantization is a method to reduce the number of bits used to represent each parameter in the model. There are three main purposes of quantization:\n",
    "1. Reduce Model Size\n",
    "Memory Efficiency: Instead of using 32-bit floating-point numbers (FP32), quantization typically reduces this to 16-bit floating-point (FP16). This leads to significant reductions in the model's memory usage.\n",
    "Storage Savings: Smaller models require less storage space, which is beneficial for deploying models on devices with limited memory, such as embedded systems.\n",
    "2. Improve Computational Efficiency\n",
    "Faster Inference: Operations involving lower-bit integers (e.g., 8-bit integers) are typically faster to execute than those involving floating-point numbers. Hardware accelerators like CPUs, GPUs, and specialized AI processors often have optimized instructions for integer arithmetic, making quantized models more efficient in terms of computation.\n",
    "Reduced Bandwidth: Lower precision data requires less bandwidth, which can be advantageous for data transfer and network communication in distributed systems or edge devices.\n",
    "3. Lower Power Consumption\n",
    "Energy Efficiency: Quantized operations consume less power compared to their floating-point counterparts.\n",
    "Hardware Utilization: Many modern processors are designed to handle lower-precision arithmetic more efficiently, leading to lower overall power usage during model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2390a954-f45d-4ace-bbe7-057312401120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88d48991-7d4e-4a7a-a14c-3c064337b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, w, bit, symmetric=False):\n",
    "        if bit is None:\n",
    "            wq = w\n",
    "        elif bit==0:\n",
    "            wq = w*0\n",
    "        else:\n",
    "            # Build a mask to record position of zero weights\n",
    "            weight_mask = (w!= 0).int()\n",
    "            if symmetric == False:\n",
    "                # Compute alpha (scale) for dynamic scaling\n",
    "                alpha = torch.max(w) - torch.min(w)\n",
    "                # Compute beta (bias) for dynamic scaling\n",
    "                beta = torch.min(w)\n",
    "                # Scale w with alpha and beta so that all elements in ws are between 0 and 1\n",
    "                ws = (w-beta)/alpha\n",
    "                step = 2 ** (bit)-1\n",
    "                # Quantize ws with a linear quantizer to \"bit\" bits\n",
    "                R = torch.round(step*ws)/step\n",
    "                # Scale the quantized weight R back with alpha and beta\n",
    "                wq = alpha*R+beta\n",
    "            else:\n",
    "                alpha = torch.max(torch.abs(w))\n",
    "                ws = w/alpha\n",
    "                step = 2**(bit-1)-1\n",
    "                R = torch.round(step*ws)/step\n",
    "                wq = alpha*R\n",
    "            # Restore zero elements in wq \n",
    "            wq = wq*weight_mask\n",
    "        return wq\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, g):\n",
    "        return g, None, None\n",
    "\n",
    "class FP_Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, Nbits=None, symmetric=False):\n",
    "        super(FP_Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        self.Nbits = Nbits\n",
    "        self.symmetric = symmetric\n",
    "        \n",
    "        m = self.in_features\n",
    "        n = self.out_features\n",
    "        self.linear.weight.data.normal_(0, math.sqrt(2. / (m+n)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, STE.apply(self.linear.weight, self.Nbits, self.symmetric), self.linear.bias)\n",
    "\n",
    "class FP_Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=False, Nbits=None, symmetric=False):\n",
    "        super(FP_Conv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n",
    "        self.Nbits = Nbits\n",
    "        self.symmetric = symmetric\n",
    "\n",
    "        n = self.kernel_size * self.kernel_size * self.out_channels\n",
    "        m = self.kernel_size * self.kernel_size * self.in_channels\n",
    "        self.conv.weight.data.normal_(0, math.sqrt(2. / (n+m) ))\n",
    "        self.sparsity = 1.0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(x, STE.apply(self.conv.weight, self.Nbits, self.symmetric), self.conv.bias, self.conv.stride, self.conv.padding, self.conv.dilation, self.conv.groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f69485a4-5f13-45ca-950f-aae447bd7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "  def __init__(self, in_channels, out_channels, stride, Nbits=None, symmetric=False):\n",
    "    super(ResidualBlock, self).__init__()\n",
    "    self.conv1 = FP_Conv(in_channels, out_channels, 3, stride=stride, padding=1, bias=False, Nbits=Nbits, symmetric=symmetric)\n",
    "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "    self.conv2 = FP_Conv(out_channels, out_channels, 3, stride=1, padding=1, bias=False, Nbits=Nbits, symmetric=symmetric)\n",
    "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    self.downsample = None\n",
    "    if stride==2:\n",
    "      self.downsample = FP_Conv(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "\n",
    "  def forward(self, x):\n",
    "    res = x\n",
    "    out = self.conv1(x)\n",
    "    out = self.bn1(out)\n",
    "    out = F.relu(out)\n",
    "    out = self.conv2(out)\n",
    "    out = self.bn2(out)\n",
    "    if self.downsample is not None:\n",
    "      res = self.downsample(x)\n",
    "    out += res\n",
    "    out = F.relu(out)\n",
    "    return out\n",
    "\n",
    "class ResNet20(nn.Module):\n",
    "  def __init__(self, resblock, n=3,  Nbits=None, symmetric=False):\n",
    "    super(ResNet20, self).__init__()\n",
    "    self.n = n\n",
    "    self.conv1 = FP_Conv(3, 16, 3, stride=1, padding=1, bias=False, Nbits=Nbits, symmetric=symmetric)\n",
    "    self.bn1 = nn.BatchNorm2d(16)\n",
    "    self.avgpool = nn.AvgPool2d(8)\n",
    "    self.fc1   = FP_Linear(64, 10, Nbits=None)\n",
    "    self.layer1 = self.create_layer(resblock, 16, 16, stride=1)\n",
    "    self.layer2 = self.create_layer(resblock, 16, 32, stride=2)\n",
    "    self.layer3 = self.create_layer(resblock, 32, 64, stride=2)\n",
    "\n",
    "  def create_layer(self, resblock, in_channels, out_channels, stride):\n",
    "    layers = nn.ModuleList()\n",
    "    layers.append(resblock(in_channels, out_channels, stride))\n",
    "    for i in range(self.n-1):\n",
    "      layers.append(resblock(out_channels, out_channels, stride=1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.conv1(x)\n",
    "    out = self.bn1(out)\n",
    "    out = F.relu(out)\n",
    "    out = self.layer1(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.layer3(out)\n",
    "    out = self.avgpool(out)\n",
    "    out = out.view(out.size(0), -1)\n",
    "    out = self.fc1(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "190f02de-d673-42a2-80c6-a850baae30b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "           FP_Conv-1           [-1, 16, 32, 32]               0\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "           FP_Conv-3           [-1, 16, 32, 32]               0\n",
      "       BatchNorm2d-4           [-1, 16, 32, 32]              32\n",
      "           FP_Conv-5           [-1, 16, 32, 32]               0\n",
      "       BatchNorm2d-6           [-1, 16, 32, 32]              32\n",
      "     ResidualBlock-7           [-1, 16, 32, 32]               0\n",
      "           FP_Conv-8           [-1, 16, 32, 32]               0\n",
      "       BatchNorm2d-9           [-1, 16, 32, 32]              32\n",
      "          FP_Conv-10           [-1, 16, 32, 32]               0\n",
      "      BatchNorm2d-11           [-1, 16, 32, 32]              32\n",
      "    ResidualBlock-12           [-1, 16, 32, 32]               0\n",
      "          FP_Conv-13           [-1, 16, 32, 32]               0\n",
      "      BatchNorm2d-14           [-1, 16, 32, 32]              32\n",
      "          FP_Conv-15           [-1, 16, 32, 32]               0\n",
      "      BatchNorm2d-16           [-1, 16, 32, 32]              32\n",
      "    ResidualBlock-17           [-1, 16, 32, 32]               0\n",
      "          FP_Conv-18           [-1, 32, 16, 16]               0\n",
      "      BatchNorm2d-19           [-1, 32, 16, 16]              64\n",
      "          FP_Conv-20           [-1, 32, 16, 16]               0\n",
      "      BatchNorm2d-21           [-1, 32, 16, 16]              64\n",
      "          FP_Conv-22           [-1, 32, 16, 16]               0\n",
      "    ResidualBlock-23           [-1, 32, 16, 16]               0\n",
      "          FP_Conv-24           [-1, 32, 16, 16]               0\n",
      "      BatchNorm2d-25           [-1, 32, 16, 16]              64\n",
      "          FP_Conv-26           [-1, 32, 16, 16]               0\n",
      "      BatchNorm2d-27           [-1, 32, 16, 16]              64\n",
      "    ResidualBlock-28           [-1, 32, 16, 16]               0\n",
      "          FP_Conv-29           [-1, 32, 16, 16]               0\n",
      "      BatchNorm2d-30           [-1, 32, 16, 16]              64\n",
      "          FP_Conv-31           [-1, 32, 16, 16]               0\n",
      "      BatchNorm2d-32           [-1, 32, 16, 16]              64\n",
      "    ResidualBlock-33           [-1, 32, 16, 16]               0\n",
      "          FP_Conv-34             [-1, 64, 8, 8]               0\n",
      "      BatchNorm2d-35             [-1, 64, 8, 8]             128\n",
      "          FP_Conv-36             [-1, 64, 8, 8]               0\n",
      "      BatchNorm2d-37             [-1, 64, 8, 8]             128\n",
      "          FP_Conv-38             [-1, 64, 8, 8]               0\n",
      "    ResidualBlock-39             [-1, 64, 8, 8]               0\n",
      "          FP_Conv-40             [-1, 64, 8, 8]               0\n",
      "      BatchNorm2d-41             [-1, 64, 8, 8]             128\n",
      "          FP_Conv-42             [-1, 64, 8, 8]               0\n",
      "      BatchNorm2d-43             [-1, 64, 8, 8]             128\n",
      "    ResidualBlock-44             [-1, 64, 8, 8]               0\n",
      "          FP_Conv-45             [-1, 64, 8, 8]               0\n",
      "      BatchNorm2d-46             [-1, 64, 8, 8]             128\n",
      "          FP_Conv-47             [-1, 64, 8, 8]               0\n",
      "      BatchNorm2d-48             [-1, 64, 8, 8]             128\n",
      "    ResidualBlock-49             [-1, 64, 8, 8]               0\n",
      "        AvgPool2d-50             [-1, 64, 1, 1]               0\n",
      "        FP_Linear-51                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 1,376\n",
      "Trainable params: 1,376\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.63\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 3.64\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = ResNet20(ResidualBlock).to(device)\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97b6da2a-fa61-47fb-9441-93c7e3a28335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"resnet_quantization.pth\")['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d59e46-0802-4123-98dc-d1f96bdbc91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop(size=(32, 32), padding=4),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "])\n",
    "\n",
    "CIFAR10_train = torchvision.datasets.CIFAR10(root='CIFAR10_data/',\n",
    "                                   train=True,\n",
    "                                   transform=train_transform,\n",
    "                                   download=True)\n",
    "\n",
    "CIFAR10_test = torchvision.datasets.CIFAR10(root='CIFAR10_data/',\n",
    "                         train=False,\n",
    "                         transform=test_transform,\n",
    "                         download=True)\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=CIFAR10_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=CIFAR10_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "def test_CIFAR10(model, test_loader, device, verbose=True):\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    model.eval()\n",
    "    total_examples = 0\n",
    "    correct_examples = 0\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            out = model(inputs)\n",
    "            loss = criterion(out, targets)\n",
    "            total_test_loss += loss.item()\n",
    "            _, predicted = torch.max(out, 1)\n",
    "            total_examples += targets.size(0)\n",
    "            correct_examples += (predicted == targets).sum().item()\n",
    "    test_avg_acc = correct_examples / total_examples\n",
    "    test_avg_loss = total_test_loss / len(test_loader)\n",
    "    if verbose:\n",
    "        print(\"Test accuracy: %.4f\" % (test_avg_acc))\n",
    "        print(\"Test loss: %.4f\" % (test_avg_loss))\n",
    "    return test_avg_acc, test_avg_loss\n",
    "    \n",
    "def plot_acc(x, acc, x_label, y_label, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(x, acc, 'b-')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c453d7b8-363c-4fbc-a94e-ac9635ad3097",
   "metadata": {},
   "source": [
    "## Fixed-point quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ad5df-6240-467f-82bf-4adc9df05191",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbits = 4\n",
    "model = ResNetCIFAR(num_layers=20, Nbits=Nbits, symmetric=False)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"resnet_quantization.pth\"))\n",
    "print('Test Accuracy: ', test_CIFAR10(model, test_loader, device)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32686f25-8dae-4258-9a02-5bfd40cf28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune(net, epochs=20, batch_size=256, lr=0.002, reg=1e-4)   \n",
    "print('Test Accuracy: ', test_CIFAR10(model, test_loader, device)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9847972d-b823-4c2a-9afa-19aa1738da44",
   "metadata": {},
   "source": [
    "## Symmetric quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf761f92-61d7-4cee-9f4a-edcb0f54ab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbits = 4\n",
    "\n",
    "model = ResNetCIFAR(num_layers=20, Nbits=Nbits, symmetric=False)\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(\"resnet_quantization.pth\"))\n",
    "print('Test Accuracy: ', test_CIFAR10(model, test_loader, device)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e71cbd-fc5a-4919-a078-89b6705fa4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe54a61f-ca01-4c9c-ac99-79139bdd3ed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be8994-659f-4d25-8a60-7cfdbefd6226",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfff074-cf5f-4ad5-b755-0d2ca693fda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e7e71-68f5-4969-ae54-e5a9208d0f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19abe8e9-10ec-491e-aa52-11cb8c89f4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
