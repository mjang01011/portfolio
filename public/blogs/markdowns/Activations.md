# Activation Functions

The Universal Approximation Theorem states that a sufficiently large MLP with one hidden layer and non-linear activation functions can approximate any continuous function. However, this wouldn't be possible with just linear activation. Imagine stacking multiple linear layers. Each layer just performs a linear combination of the previous layer's outputs, so the entire network would essentially be equivalent to a single linear layer, limiting it to representing only linear relationships. Therefore, with non-linear activation, we can represent the complex non-linear relationship between the input and the output and represent a much more complicated function with deeper non-linear layers.

# Logits

https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean

# Vanishing Gradient

We can't simply make our network deeper. The first issue is the vanishing gradient problem. 

# Batch Norm

https://www.youtube.com/watch?v=JJZZWM9tGp4

# Layer Norm

# Loss Landscape

# Dropout

# Regularization

