# Reinforcement Learning with Human Feedback (RLHF)

To get a detailed overview of RLHF, see this paper from OpenAI: Long Ouyang et al. *Training language models to follow instructions with human feedback.* 2022. [arXiv:2203.02155](https://arxiv.org/abs/2203.02155).

## What is & Why RLHF?

Language models, even after extensive training, can exhibit undesirable behaviors. This can manifest in outputs that are not helpful, honest, or harmless. RLHF is a critical process used to fine-tune language models so that they behave in a way that aligns with human values and expectations.

 The RLHF process typically involves three key stages:

![img](https://miro.medium.com/v2/resize:fit:1050/0*hMyazpDlQ3oHb1bg.jpg)

1. Supervised Fine-Tuning (SFT): To teach the model the policies intended by humans, a pre-trained language model is fine-tuned using a small dataset selected by a human labeler (or trainer).
2. Reward Model Training (Mimic Human Preferences): Human labelers collect a dataset by ranking the multiple response candidates generated by the fine-tuned SFT model in step 1, according to which responses they find better. This dataset is then used to train a new reward model. The reason why we rank the responses is that giving a numerical score for each response is highly subjective, and different labelers might have different baselines or standards for what constitutes a "good" or "bad" response. Ranking eliminates some of this subjectivity by focusing on the relative quality of the responses.
3. Fine-tuning with Reinforcement Learning (RL): The SFT model is given various user inputs and interacts with the reward model to perform reinforcement learning.

## Reinforcement Learning & Preparing Data

Reinforcement Learning (RL) is a type of machine learning where the objective is to maximize the reward received for actions. In the context of large language models (LLMs), this involves generating text that is aligned with human expectations. 

### Key Concepts:

- **Agent/Policy**: In RL, the language model itself (LLM) acts as the agent or policy that takes actions to maximize rewards.
- **Environment**: The environment is the context provided to the LLM.
- **Actions**: The actions taken by the LLM are the tokens generated as output.
- **State**: The state refers to the current context within which the model is operating.

### Measuring Output Quality

To decide if the output generated by the LLM is actually good, we need a way to measure its quality. While humans can do this, it's not scalable. This is where a reward model comes in.

- **Reward Model**: This model ranks the order of outputs generated by the LLM. By comparing pairs of outputs, the reward model learns which output is preferred. The goal is for the reward model to predict which completion (output) is better, given a prompt.

- **Training Process**: The reward model is trained to predict the preferred completion (y1 or y2) based on a given prompt (x). The logit value (before it's converted into a probability via softmax) is used as the reward.

## PPO: Proximal Policy Optimization

Proximal Policy Optimization (PPO) is a popular algorithm used for fine-tuning language models with RL. However, RL can introduce potential issues, such as reward hacking.

### Potential Issue: Reward Hacking

Reward hacking occurs when the LLM learns to generate outputs that maximize the reward regardless of whether they actually align with the desired behavior. For example, the LLM might learn to always generate positive-sounding completions to increase its reward score, even if the completions are not contextually appropriate.

### Solution: KL Divergence Penalty

To mitigate reward hacking, a KL Divergence penalty can be introduced. This involves:

1. **Freezing a Reference Model**: A reference model is frozen (not updated during training), and the RL-updated LLM is compared against it.
2. **KL Divergence**: The KL Divergence measures how much the updated model's output distribution diverges from the reference model's distribution. A penalty is added to the reward model to prevent the RL-updated model from deviating too far from the reference model.

### Parameter Efficient Fine-Tuning (PEFT)

After applying PPO, PEFT techniques can be used to update only a small subset of the model's parameters. This approach helps in efficiently fine-tuning the model without requiring updates to all of its parameters.

### Evaluating the Human-Aligned LLM

Finally, to evaluate the success of the RLHF process, we can assess the LLM's outputs using a toxicity score or other metrics designed to measure how well the model aligns with human values.
